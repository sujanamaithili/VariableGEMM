#pragma once

#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <limits>
#include <map>
#include <tuple>
#include <vector>

#include <cublas_v2.h>
#include <cuda_runtime.h>

#include "ops/op_reduction.cuh"
#include "utils/assert.cuh"
#include "utils/cublas_utils.h"
#include "utils/tensor.cuh"


template <typename T>
__global__ void op_mm_kernel(const Tensor<T> A, const Tensor<T> B,
                             Tensor<T> C) {
  int row = blockIdx.x;
  int col = threadIdx.x;
  T temp = 0;
  for (int i = 0; i < A.w; i++) {
    temp += Index(A, row, i) * Index(B, i, col);
  }
  Index(C, row, col) = temp;
}

// This operator compute C = A@B
template <typename T>
void op_mm_old(const Tensor<T> &A, const Tensor<T> &B, Tensor<T> &C) {
   assert(A.h == C.h && B.w == C.w && A.w == B.h);
   assert(A.on_device && B.on_device && C.on_device);

   // Lab-1: please complete this
   // You need to define separate kernel function(s) and launch them here
   // delete assert(0) when you are finished
   int blockSize = C.h;
   int numBlocks = C.w;
   op_mm_kernel<<<blockSize, numBlocks>>>(A, B, C);
}

template <typename T>
void op_mm(const Tensor<T> &A, const Tensor<T> &B, Tensor<T> &C) {
  assert(A.w == B.h && A.h == C.h && B.w == C.w);
  assert(A.on_device && B.on_device && C.on_device);
  int m = A.h, k = A.w, n = B.w;
  int lda = A.h, ldb = B.h, ldc = C.h;
  cublasOperation_t transa = CUBLAS_OP_N;
  cublasOperation_t transb = CUBLAS_OP_N;
  const T alpha = 1.0;
  const T beta = 0.0;
	
  T *A_rowMajor = (T *)malloc(sizeof(T) * A.h * A.w);
  T *B_rowMajor = (T *)malloc(sizeof(T) * B.h * B.w);
  cudaAssert(cudaMemcpy(A_rowMajor, A.rawp, A.h * A.w * sizeof(T), cudaMemcpyDeviceToHost));
  cudaAssert(cudaMemcpy(B_rowMajor, B.rawp, B.h * B.w * sizeof(T), cudaMemcpyDeviceToHost));
	
  T *A_colMajor = (T *)malloc(sizeof(T) * A.h * A.w);
  T *B_colMajor = (T *)malloc(sizeof(T) * B.h * B.w);
  T *C_colMajor = (T *)malloc(sizeof(T) * C.h * C.w);
  for (int row = 0; row < A.h; row++) {
    for (int col = 0; col < A.w; col++) {
//      A_colMajor[row + col * A.h] = (((A_rowMajor)[(A).offset + (row) * (A).stride_h + (col) * A.stride_w]));
			A_colMajor[row + col * A.h] = A_rowMajor[row * A.w + col];
    }
  }

  for (int row = 0; row < B.h; row++) {
    for (int col = 0; col < B.w; col++) {
//      B_colMajor[row + col * B.h] = (((B_rowMajor)[(B).offset + (row) * (B).stride_h + (col) * B.stride_w]));
			B_colMajor[row + col * A.h] = B_rowMajor[row * B.w + col];
    }
  }
  /* 
  std::cout << "A ---" << std::endl;
  for (int i = 0; i < A.h * A.w; i++) {
  	if (i % A.w == 0)   std::cout << std::endl;
	  std::cout << A_rowMajor[i] << ' ';
  }
  std::cout << std::endl;
  std::cout << "B ---" << std::endl;
  for (int i = 0; i < B.h * B.w; i++) {
    if (i % B.w == 0)   std::cout << std::endl;
		std::cout << B_rowMajor[i] << ' ';
  }
  std::cout << std::endl;
	*/ 
  delete[] A_rowMajor;
  delete[] B_rowMajor;
  
  cublasHandle_t cublasH = NULL;
  cudaStream_t stream = NULL;

  T *d_A = nullptr;
  T *d_B = nullptr;
  T *d_C = nullptr;

  CUBLAS_CHECK(cublasCreate(&cublasH));
  CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));
  CUBLAS_CHECK(cublasSetStream(cublasH, stream));

  CUDA_CHECK(
      cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(T) * A.h * A.w));
  CUDA_CHECK(
      cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(T) * B.h * B.w));
  CUDA_CHECK(
      cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(T) * C.h * C.w));

  CUDA_CHECK(cudaMemcpyAsync(d_A, A_colMajor, sizeof(T) * A.h * A.w,
                             cudaMemcpyHostToDevice, stream));
  CUDA_CHECK(cudaMemcpyAsync(d_B, B_colMajor, sizeof(T) * B.h * B.w,
                               cudaMemcpyHostToDevice, stream));
  delete[] A_colMajor;
  delete[] B_colMajor;
	CUDA_CHECK(cudaStreamSynchronize(stream));
  CUBLAS_CHECK(cublasSgemm(cublasH, transa, transb, m, n, k, &alpha, d_A, lda,
                           d_B, ldb, &beta, d_C, ldc));
  CUDA_CHECK(cudaMemcpyAsync(C_colMajor, d_C, sizeof(T) * C.h * C.w, cudaMemcpyDeviceToHost, stream));
  CUDA_CHECK(cudaStreamSynchronize(stream));
  CUDA_CHECK(cudaFree(d_A));
  CUDA_CHECK(cudaFree(d_B));
  CUDA_CHECK(cudaFree(d_C));

  CUBLAS_CHECK(cublasDestroy(cublasH));
  CUDA_CHECK(cudaStreamDestroy(stream));
  T *C_rowMajor = (T *)malloc(sizeof(T) * C.h * C.w);
  for (int col = 0; col < C.w; col++) {
    for (int row = 0; row < C.h; row++) {
      C_rowMajor[row * C.w + col] = (T)C_colMajor[row + col * C.h];
    }
  }
/*	
	std::cout << "C ---" << std::endl;
  for (int i = 0; i < C.h * C.w; i++) {
    if (i % C.w == 0)   std::cout << std::endl;
		std::cout << C_rowMajor[i] << ' ';
  }
  std::cout << std::endl;
*/	
  cudaAssert(cudaMemcpy(C.rawp, C_rowMajor, C.h * C.w * sizeof(T), cudaMemcpyHostToDevice));
  delete[] C_colMajor;
  delete[] C_rowMajor;
/*
  Tensor<T> C_old{C.h, C.w, true};
	 op_mm_old(A, B, C_old);
   Tensor<int> correct_preds{C.h, C.w, true};
   op_equal(C, C_old, correct_preds);
   Tensor<int> sum_correct1{1,C.w, true};
  	Tensor<int> sum_correct2{1, 1, true};
   op_sum(correct_preds, sum_correct1);
   op_sum(sum_correct1, sum_correct2);
 	 auto tmp = sum_correct2.toHost();
 	 if (Index(tmp, 0, 0) == C.h * C.w) {
    std::cout << "mm correct" << std::endl;
 	 } else {
    std::cout << "mm failed" << std::endl;
   }
	C = C.toHost();
	C_old = C_old.toHost();
	for (int i = 0; i < C.h * C.w; i++) 	std::cout << (float)C.rawp[i] << ' '; std::cout << std::endl;
	for (int i = 0; i < C.h * C.w; i++)   std::cout << (float)C_old.rawp[i] << ' '; std::cout << std::endl;
	C = C.toDevice();
  */
	// std::cout << "op mm completed" << std::endl;
	assert(A.on_device && B.on_device && C.on_device);
}

template <typename T>
int cublas_gemm(int m_array[], int n_array[], int k_array[], T alpha_array[],
                int lda_array[], int ldb_array[], T beta_array[],
                int ldc_array[], int group_count, int group_size[],
                const std::vector<std::vector<T>> &A_array,
                const std::vector<std::vector<T>> &B_array,
                std::vector<std::vector<T>> &C_array) {
  cublasHandle_t cublasH = NULL;
  cudaStream_t stream = NULL;

  const int gemm_count = A_array.size();
  std::vector<T *> d_A(gemm_count, nullptr);
  std::vector<T *> d_B(gemm_count, nullptr);
  std::vector<T *> d_C(gemm_count, nullptr);

  T **d_A_array = nullptr;
  T **d_B_array = nullptr;
  T **d_C_array = nullptr;

  cublasOperation_t transa_array[group_count] = {CUBLAS_OP_N, CUBLAS_OP_N};
  cublasOperation_t transb_array[group_count] = {CUBLAS_OP_N, CUBLAS_OP_N};
  int problem_idx = 0;
  for (int i = 0; i < group_count; i++) {
    printf("Group %d:\n", i);
    for (int j = 0; j < group_size[i]; j++) {
      printf("A[%d]\n", j);
      print_matrix(m_array[i], k_array[i], A_array[problem_idx].data(),
                   lda_array[i]);
      printf("=====\n");

      printf("B[%d]\n", j);
      print_matrix(k_array[i], n_array[i], B_array[problem_idx].data(),
                   ldb_array[i]);
      printf("=====\n");

      problem_idx++;
    }
    printf("\n");
  }
  /* step 1: create cublas handle, bind a stream */
  CUBLAS_CHECK(cublasCreate(&cublasH));

  CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));
  CUBLAS_CHECK(cublasSetStream(cublasH, stream));

  /* step 2: copy data to device */
  for (int i = 0; i < gemm_count; i++) {
    CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A[i]),
                          sizeof(T) * A_array[i].size()));
    CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B[i]),
                          sizeof(T) * B_array[i].size()));
    CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C[i]),
                          sizeof(T) * C_array[i].size()));
  }

  CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_array),
                        sizeof(T *) * gemm_count));
  CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B_array),
                        sizeof(T *) * gemm_count));
  CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C_array),
                        sizeof(T *) * gemm_count));

  for (int i = 0; i < gemm_count; i++) {
    CUDA_CHECK(cudaMemcpyAsync(d_A[i], A_array[i].data(),
                               sizeof(T) * A_array[i].size(),
                               cudaMemcpyHostToDevice, stream));
    CUDA_CHECK(cudaMemcpyAsync(d_B[i], B_array[i].data(),
                               sizeof(T) * B_array[i].size(),
                               cudaMemcpyHostToDevice, stream));
  }

  CUDA_CHECK(cudaMemcpyAsync(d_A_array, d_A.data(),
                             sizeof(T *) * gemm_count,
                             cudaMemcpyHostToDevice, stream));
  CUDA_CHECK(cudaMemcpyAsync(d_B_array, d_B.data(),
                             sizeof(T *) * gemm_count,
                             cudaMemcpyHostToDevice, stream));
  CUDA_CHECK(cudaMemcpyAsync(d_C_array, d_C.data(),
                             sizeof(T *) * gemm_count,
                             cudaMemcpyHostToDevice, stream));

  /* step 3: compute */
  CUBLAS_CHECK(cublasSgemmGroupedBatched(
      cublasH, transa_array, transb_array, m_array, n_array, k_array,
      alpha_array, d_A_array, lda_array, d_B_array, ldb_array, beta_array,
      d_C_array, ldc_array, group_count, group_size));

  /* step 4: copy data to host */
  for (int i = 0; i < gemm_count; i++) {
    CUDA_CHECK(cudaMemcpyAsync(C_array[i].data(), d_C[i],
                               sizeof(T) * C_array[i].size(),
                               cudaMemcpyDeviceToHost, stream));
  }

  CUDA_CHECK(cudaStreamSynchronize(stream));

  problem_idx = 0;
  for (int i = 0; i < group_count; i++) {
    printf("Group %d:\n", i);
    for (int j = 0; j < group_size[i]; j++) {
      printf("C[%d]\n", j);
      print_matrix(m_array[i], n_array[i], C_array[problem_idx].data(),
                   ldc_array[i]);
      printf("=====\n");

      problem_idx++;
    }
    if (i < group_count - 1) {
      printf("\n");
    }
  }

  /* free resources */
  CUDA_CHECK(cudaFree(d_A_array));
  CUDA_CHECK(cudaFree(d_B_array));
  CUDA_CHECK(cudaFree(d_C_array));
  for (int i = 0; i < gemm_count; i++) {
    CUDA_CHECK(cudaFree(d_A[i]));
    CUDA_CHECK(cudaFree(d_B[i]));
    CUDA_CHECK(cudaFree(d_C[i]));
  }

  CUBLAS_CHECK(cublasDestroy(cublasH));

  CUDA_CHECK(cudaStreamDestroy(stream));

  // CUDA_CHECK(cudaDeviceReset());

  return EXIT_SUCCESS;
}

template <typename T>
void rowMajor_to_colMajor(const Tensor<T> &src, std::vector<T> &dest) {
  printf("allocating memory in dest\n");
	dest.resize(src.h * src.w);
	printf("copying data to destination\n");
	for (int col_idx = 0; col_idx < src.w; col_idx++) {
		for (int row_idx = 0; row_idx < src.h; row_idx++) {
			printf("idx = %d \n", col_idx * src.h + row_idx);
			printf(" val = %f\n", Index(src, row_idx, col_idx));
			dest[col_idx * src.h + row_idx] = Index(src, row_idx, col_idx);
		}
	}
  // for (int i = 0; i < src.h * src.w; i++) {
    // std::cout << src.rawp << " ";
    // dest[i] = src.rawp[i];
  // }
  std::cout << std::endl;
}

template <typename T>
void batched_gemm(const std::vector<Tensor<T>> &A_batched,
                  const std::vector<Tensor<T>> &B_batched,
                  std::vector<Tensor<T>> &C_batched) {
  printf("starting batched gemm\n");
	int gemm_count = A_batched.size();

  std::map<std::tuple<int, int, int>, std::vector<int>> size_map;
  for (int i = 0; i < gemm_count; i++) {
		printf("%d --> (%d, %d) x (%d, %d)\n", i, A_batched[i].h, A_batched[i].w, B_batched[i].h, B_batched[i].w);
    size_map[std::make_tuple(A_batched[i].h, A_batched[i].w, B_batched[i].w)]
        .push_back(i);
  }
	for (auto it : size_map) {
		printf("group\n");
		for (auto idx : it.second) {
			printf("%d %p\t", idx, A_batched[idx].rawp);
		}
		printf("\n");
	}
  int group_count = std::abs(std::distance(size_map.begin(), size_map.end()));
	printf("divided input into groups\n");
  int lda_array[group_count], ldb_array[group_count], ldc_array[group_count];
  int m_array[group_count], k_array[group_count], n_array[group_count];
  std::vector<std::vector<T>> A_array(gemm_count), B_array(gemm_count),
      C_array(gemm_count);
  int group_size[group_count];
  T alpha_array[group_count], beta_array[group_count];

  int problem_idx = 0;
  int group_idx = 0;
  for (auto it : size_map) {
		printf("arranging data for group #%d\n", group_idx);
    for (auto idx : it.second) {
			printf("converting rowMajor to colMajor of input #%d\n", problem_idx);
			printf("Tensor A = %s\n", A_batched[idx].str());
			rowMajor_to_colMajor<T>(A_batched[idx], A_array[problem_idx]);
      rowMajor_to_colMajor<T>(B_batched[idx], B_array[problem_idx]);
      rowMajor_to_colMajor<T>(C_batched[idx], C_array[problem_idx]);
      problem_idx++;
    }
    lda_array[group_idx] = std::get<0>(it.first);
    ldb_array[group_idx] = std::get<1>(it.first);
    ldc_array[group_idx] = std::get<2>(it.first);

    m_array[group_idx] = std::get<0>(it.first);
    k_array[group_idx] = ldb_array[group_idx];
    n_array[group_idx] = ldc_array[group_idx];

    group_size[group_idx] = it.second.size();
    alpha_array[group_idx] = 1.0;
    beta_array[group_idx] = 0.0;
    group_idx++;
  }
	printf("calling cublas gemm\n");
  cublas_gemm<T>(m_array, n_array, k_array, alpha_array, lda_array, ldb_array,
                 beta_array, ldc_array, group_count, group_size, A_array,
                 B_array, C_array);
	printf("reorganizing output into original order\n");
  problem_idx = 0;
  for (auto it : size_map) {
    for (auto idx : it.second) {
      for (int i = 0; i < C_array[problem_idx].size(); i++) {
        C_batched[idx].rawp[i] = C_array[problem_idx][i];
      }
      problem_idx++;
    }
  }
}
